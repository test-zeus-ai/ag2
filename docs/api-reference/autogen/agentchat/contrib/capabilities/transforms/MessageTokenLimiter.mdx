---
sidebarTitle: MessageTokenLimiter
title: autogen.agentchat.contrib.capabilities.transforms.MessageTokenLimiter
---
<h2 id="autogen.agentchat.contrib.capabilities.transforms.MessageTokenLimiter" class="doc doc-heading">
    <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
    <span class="doc doc-object-name doc-class-name">MessageTokenLimiter</span>
</h2>

```python
MessageTokenLimiter(
    max_tokens_per_message: int | None = None,
    max_tokens: int | None = None,
    min_tokens: int | None = None,
    model: str = 'gpt-3.5-turbo-0613',
    filter_dict: dict | None = None,
    exclude_filter: bool = True
)
```

    Truncates messages to meet token limits for efficient processing and response generation.
    
    This transformation applies two levels of truncation to the conversation history:
    
    1. Truncates each individual message to the maximum number of tokens specified by max_tokens_per_message.
    2. Truncates the overall conversation history to the maximum number of tokens specified by max_tokens.
    
    NOTE: Tokens are counted using the encoder for the specified model. Different models may yield different token
        counts for the same text.
    
    NOTE: For multimodal LLMs, the token count may be inaccurate as it does not account for the non-text input
        (e.g images).
    
    The truncation process follows these steps in order:
    
    1. The minimum tokens threshold (`min_tokens`) is checked (0 by default). If the total number of tokens in messages
        are less than this threshold, then the messages are returned as is. In other case, the following process is applied.
    2. Messages are processed in reverse order (newest to oldest).
    3. Individual messages are truncated based on max_tokens_per_message. For multimodal messages containing both text
        and other types of content, only the text content is truncated.
    4. The overall conversation history is truncated based on the max_tokens limit. Once the accumulated token count
        exceeds this limit, the current message being processed get truncated to meet the total token count and any
        remaining messages get discarded.
    5. The truncated conversation history is reconstructed by prepending the messages to a new list to preserve the
        original message order.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `max_tokens_per_message` | Maximum number of tokens to keep in each message.<br/><br/>Must be greater than or equal to 0 if not None.<br/><br/>**Type:** `int \| None`<br/><br/>**Default:** None |
| `max_tokens` | Maximum number of tokens to keep in the chat history.<br/><br/>Must be greater than or equal to 0 if not None.<br/><br/>**Type:** `int \| None`<br/><br/>**Default:** None |
| `min_tokens` | Minimum number of tokens in messages to apply the transformation.<br/><br/>Must be greater than or equal to 0 if not None.<br/><br/>**Type:** `int \| None`<br/><br/>**Default:** None |
| `model` | The target OpenAI model for tokenization alignment.<br/><br/>**Type:** `str`<br/><br/>**Default:** 'gpt-3.5-turbo-0613' |
| `filter_dict` | A dictionary to filter out messages that you want/don't want to compress.<br/><br/>If None, no filters will be applied.<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** None |
| `exclude_filter` | If exclude filter is True (the default value), messages that match the filter will be excluded from token truncation.<br/><br/>If False, messages that match the filter will be truncated.<br/><br/>**Type:** `bool`<br/><br/>**Default:** True |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### apply_transform

```python
apply_transform(self, messages: list[dict]) -> list[dict]
```

    Applies token truncation to the conversation history.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `messages` | The list of messages representing the conversation history.<br/><br/>**Type:** `list[dict]` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `list[dict]` | List[Dict]: A new list containing the truncated messages up to the specified token limits. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_logs

```python
get_logs(
    self,
    pre_transform_messages: list[dict],
    post_transform_messages: list[dict]
) -> tuple[str, bool]
```

    

<b>Parameters:</b>
| Name | Description |
|--|--|
| `pre_transform_messages` | **Type:** `list[dict]` |
| `post_transform_messages` | **Type:** `list[dict]` |

<br />