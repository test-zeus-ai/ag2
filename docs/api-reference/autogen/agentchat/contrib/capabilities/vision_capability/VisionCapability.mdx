---
sidebarTitle: VisionCapability
title: autogen.agentchat.contrib.capabilities.vision_capability.VisionCapability
---
<h2 id="autogen.agentchat.contrib.capabilities.vision_capability.VisionCapability" class="doc doc-heading">
    <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
    <span class="doc doc-object-name doc-class-name">VisionCapability</span>
</h2>

```python
VisionCapability(
    lmm_config: dict,
    description_prompt: str | None = 'Write a detailed caption for this image. Pay special attention to any details that might be useful or relevant to the ongoing conversation.',
    custom_caption_func: Callable = None
)
```

    We can add vision capability to regular ConversableAgent, even if the agent does not have the multimodal capability,
    such as GPT-3.5-turbo agent, Llama, Orca, or Mistral agents. This vision capability will invoke a LMM client to describe
    the image (captioning) before sending the information to the agent's actual client.
    
        The vision capability will hook to the ConversableAgent's `process_last_received_message`.
    
        Some technical details:
        When the agent (who has the vision capability) received an message, it will:
        1. _process_received_message:
            a. _append_oai_message
        2. generate_reply: if the agent is a MultimodalAgent, it will also use the image tag.
            a. hook process_last_received_message (NOTE: this is where the vision capability will be hooked to.)
            b. hook process_all_messages_before_reply
        3. send:
            a. hook process_message_before_send
            b. _append_oai_message
    
    Initializes a new instance, setting up the configuration for interacting with
    a Language Multimodal (LMM) client and specifying optional parameters for image
    description and captioning.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `lmm_config` | Configuration for the LMM client, which is used to call the LMM service for describing the image.<br/><br/>This must be a dictionary containing the necessary configuration parameters.<br/><br/>If `lmm_config` is False or an empty dictionary, it is considered invalid, and initialization will assert.<br/><br/>**Type:** `dict` |
| `description_prompt` | The prompt to use for generating descriptions of the image.<br/><br/>This parameter allows customization of the prompt passed to the LMM service.<br/><br/>Defaults to `DEFAULT_DESCRIPTION_PROMPT` if not provided.<br/><br/>**Type:** `str \| None`<br/><br/>**Default:** 'Write a detailed caption for this image. Pay special attention to any details that might be useful or relevant to the ongoing conversation.' |
| `custom_caption_func` | A callable that, if provided, will be used to generate captions for images.<br/><br/>This allows for custom captioning logic outside of the standard LMM service interaction.<br/><br/>The callable should take three parameters as input: 1. an image URL (or local location) 2. image_data (a PIL image) 3. lmm_client (to call remote LMM) and then return a description (as string).<br/><br/>If not provided, captioning will rely on the LMM client configured via `lmm_config`.<br/><br/>If provided, we will not run the default self._get_image_caption method.<br/><br/>**Type:** `Callable`<br/><br/>**Default:** None |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### process_last_received_message

```python
process_last_received_message(self, content: list[dict] | str) -> str
```

    Processes the last received message content by normalizing and augmenting it
            with descriptions of any included images. The function supports input content
            as either a string or a list of dictionaries, where each dictionary represents
            a content item (e.g., text, image). If the content contains image URLs, it
            fetches the image data, generates a caption for each image, and inserts the
            caption into the augmented content.
    
            The function aims to transform the content into a format compatible with GPT-4V
            multimodal inputs, specifically by formatting strings into PIL-compatible
            images if needed and appending text descriptions for images. This allows for
            a more accessible presentation of the content, especially in contexts where
            images cannot be displayed directly.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `content` | The last received message content, which can be a plain text string or a list of dictionaries representing different types of content items (e.g., text, image_url).<br/><br/>**Type:** `list[dict] \| str` |

<b>Returns:</b>
| Type | Description |
|--|--|
| `str` | str: The augmented message content Raises: AssertionError: If an item in the content list is not a dictionary. Examples: Assuming `self._get_image_caption(img_data)` returns "A beautiful sunset over the mountains" for the image. - Input as String: content = "Check out this cool photo!" Output: "Check out this cool photo!" (Content is a string without an image, remains unchanged.) - Input as String, with image location: content = "What's weather in this cool photo: `&lt;img http://example.com/photo.jpg>`" Output: "What's weather in this cool photo: `&lt;img http://example.com/photo.jpg>` in case you can not see, the caption of this image is: A beautiful sunset over the mountains " (Caption added after the image) - Input as List with Text Only: content = `[\\{"type": "text", "text": "Here's an interesting fact."}]` Output: "Here's an interesting fact." (No images in the content, it remains unchanged.) - Input as List with Image URL: ```python content = [ \\{"type": "text", "text": "What's weather in this cool photo:"}, \\{"type": "image_url", "image_url": \\{"url": "http://example.com/photo.jpg"}}, ] ``` Output: "What's weather in this cool photo: `&lt;img http://example.com/photo.jpg>` in case you can not see, the caption of this image is: A beautiful sunset over the mountains " (Caption added after the image) |

<br />