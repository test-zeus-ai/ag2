---
sidebarTitle: MathUserProxyAgent
title: autogen.agentchat.contrib.math_user_proxy_agent.MathUserProxyAgent
---
<h2 id="autogen.agentchat.contrib.math_user_proxy_agent.MathUserProxyAgent" class="doc doc-heading">
    <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
    <span class="doc doc-object-name doc-class-name">MathUserProxyAgent</span>
</h2>

```python
MathUserProxyAgent(
    name: str | None = 'MathChatAgent',
    is_termination_msg: Callable[[dict], bool] | None = &lt;function _is_termination_msg_mathchat>,
    human_input_mode: Literal['ALWAYS', 'NEVER', 'TERMINATE'] = 'NEVER',
    default_auto_reply: dict | str | None = 'Continue. Please keep solving the problem until you need to query. (If you get to the answer, put it in \\boxed\{}.)',
    max_invalid_q_per_step=3,
    **kwargs
)
```

    (Experimental) A MathChat agent that can handle math problems.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `name` | name of the agent<br/><br/>**Type:** `str \| None`<br/><br/>**Default:** 'MathChatAgent' |
| `is_termination_msg` | a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.<br/><br/>The dict can contain the following keys: "content", "role", "name", "function_call".<br/><br/>**Type:** `Callable[[dict], bool] \| None`<br/><br/>**Default:** &lt;function _is_termination_msg_mathchat> |
| `human_input_mode` | whether to ask for human inputs every time a message is received.<br/><br/>Possible values are "ALWAYS", "TERMINATE", "NEVER".<br/><br/>(1) When "ALWAYS", the agent prompts for human input every time a message is received.<br/><br/>Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input.<br/><br/>(2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply.<br/><br/>(3) (Default) When "NEVER", the agent will never prompt for human input.<br/><br/>Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.<br/><br/>**Type:** `Literal['ALWAYS', 'NEVER', 'TERMINATE']`<br/><br/>**Default:** 'NEVER' |
| `default_auto_reply` | the default auto reply message when no code execution or llm based reply is generated.<br/><br/>**Type:** `dict \| str \| None`<br/><br/>**Default:** 'Continue. Please keep solving the problem until you need to query. (If you get to the answer, put it in \\boxed\{}.)' |
| `max_invalid_q_per_step=3` |  |
| `**kwargs` | other kwargs in [UserProxyAgent](../user_proxy_agent#init).<br/><br/> |

### Class Attributes

<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>
#### DEFAULT_REPLY
<br />

    <br />

<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>
#### MAX_CONSECUTIVE_AUTO_REPLY
<br />

    <br />

### Static Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### message_generator

```python
message_generator(
    sender,
    recipient,
    context
) -> 
```

    Generate a prompt for the assistant agent with the given problem and prompt.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `sender` | the sender of the message.<br/><br/> |
| `recipient` | the recipient of the message.<br/><br/> |
| `context` | a dictionary with the following fields: problem (str): the problem to be solved.<br/><br/>prompt_type (str, Optional): the type of the prompt.<br/><br/>Possible values are "default", "python", "wolfram".<br/><br/>(1) "default": the prompt that allows the agent to choose between 3 ways to solve a problem: 1. write a python program to solve it directly.<br/><br/>2. solve it directly without python.<br/><br/>3. solve it step by step with python.<br/><br/>(2) "python": a simplified prompt from the third way of the "default" prompt, that asks the assistant to solve the problem step by step with python.<br/><br/>(3) "two_tools": a simplified prompt similar to the "python" prompt, but allows the model to choose between Python and Wolfram Alpha to solve the problem.<br/><br/>customized_prompt (str, Optional): a customized prompt to be used.<br/><br/>If it is not None, the prompt_type will be ignored.<br/><br/> |

<br />

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### execute_one_python_code

```python
execute_one_python_code(self, pycode) -> 
```

    Execute python code blocks.
    
    Previous python code will be saved and executed together with the new code.
    the "print" function will also be added to the last line of the code if needed

<b>Parameters:</b>
| Name | Description |
|--|--|
| `pycode` |  |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### execute_one_wolfram_query

```python
execute_one_wolfram_query(self, query: str) -> 
```

    Run one wolfram query and return the output.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `query` | string of the query.<br/><br/>**Type:** `str` |

<br />