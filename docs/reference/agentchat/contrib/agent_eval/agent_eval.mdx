---
sidebarTitle: agent_eval
title: agentchat.contrib.agent_eval.agent_eval
---

### generate\_criteria

```python
def generate_criteria(llm_config: Optional[Union[dict, Literal[False]]] = None,
                      task: Task = None,
                      additional_instructions: str = "",
                      max_round=2,
                      use_subcritic: bool = False)
```

Creates a list of criteria for evaluating the utility of a given task.

**Arguments**:

- `llm_config` _dict or bool_ - llm inference configuration.
- `task` _Task_ - The task to evaluate.
- `additional_instructions` _str_ - Additional instructions for the criteria agent.
- `max_round` _int_ - The maximum number of rounds to run the conversation.
- `use_subcritic` _bool_ - Whether to use the subcritic agent to generate subcriteria.
  

**Returns**:

- `list` - A list of Criterion objects for evaluating the utility of the given task.

### quantify\_criteria

```python
def quantify_criteria(llm_config: Optional[Union[dict, Literal[False]]] = None,
                      criteria: list[Criterion] = None,
                      task: Task = None,
                      test_case: str = "",
                      ground_truth: str = "")
```

Quantifies the performance of a system using the provided criteria.

**Arguments**:

- `llm_config` _dict or bool_ - llm inference configuration.
- `criteria` _[Criterion]_ - A list of criteria for evaluating the utility of a given task.
- `task` _Task_ - The task to evaluate.
- `test_case` _str_ - The test case to evaluate.
- `ground_truth` _str_ - The ground truth for the test case.
  

**Returns**:

- `dict` - A dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.

